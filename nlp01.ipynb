{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df742503-01ba-461b-abd4-1496a26a3760",
   "metadata": {},
   "source": [
    "## [***Rule-based Sentiment Classifier***](https://github.com/neubig/anlp-code/tree/main/data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af1ab230-49bd-4853-b5ac-f0ddea4043ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train, test, dev data\n",
    "import os\n",
    "from pathlib import Path\n",
    "base_dir = Path(\"/home/amiagarw/01-Programming/Python/Data/sst-sentiment-text-threeclass/\")\n",
    "\n",
    "# Paths to the specific files\n",
    "dev = base_dir / \"dev.txt\"\n",
    "test = base_dir / \"test.txt\"\n",
    "train = base_dir / \"train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "489ba171-5fea-4055-968b-45aa0f761794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data from the training and dev (or test) sets\n",
    "def read_xy_data(filename: str) -> tuple[list[str], list[int]]:\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            label, text = line.strip().split(' ||| ')\n",
    "            x_data.append(text)\n",
    "            y_data.append(int(label))\n",
    "    return x_data, y_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac2dd39b-44fe-4751-b11a-6b861046dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = read_xy_data(train)\n",
    "x_test, y_test = read_xy_data(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c0b6eb-2746-41f5-b87f-325a7b64c3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train: The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n",
      "y_train: 1\n"
     ]
    }
   ],
   "source": [
    "print(f\"x_train: {x_train[0]}\")\n",
    "print(f\"y_train: {y_train[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1a8e322-5f50-49bc-92ed-f4b93bc4c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc742ef-423e-4d90-a507-cf2db0682326",
   "metadata": {},
   "source": [
    "It takes in a text X and return a label of \n",
    "- \"1\" if the sentiment of the text is positive,\n",
    "- \"-1\" if the sentiment of the text is negative, and\n",
    "- \"0\" if the sentiment of the text is neutral. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7557fc5f-1e31-4d50-a43a-5ac07cc12e45",
   "metadata": {},
   "source": [
    "***Extract_features(X)*** extracts a dictionary of (named) feature values from the text.\n",
    "Create this by hand, and a simple example is shown for you.\n",
    "\n",
    "***feature_weights***, a dictionary to assign a weight to each extracted feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee990794-16d2-4188-8eb2-2b9feafaa85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Original code\n",
    "# def extract_features(x: str) -> dict[str, float]:\n",
    "#     features = {}\n",
    "#     x_split = x.split(' ')\n",
    "    \n",
    "#     # Count the number of \"good words\" and \"bad words\" in the text\n",
    "#     good_words = ['love', 'good', 'nice', 'great', 'enjoy', 'enjoyed']\n",
    "#     bad_words = ['hate', 'bad', 'terrible', 'disappointing', 'sad', 'lost', 'angry']\n",
    "#     for x_word in x_split:\n",
    "#         if x_word in good_words:\n",
    "#             features['good_word_count'] = features.get('good_word_count', 0) + 1\n",
    "#         if x_word in bad_words:\n",
    "#             features['bad_word_count'] = features.get('bad_word_count', 0) + 1\n",
    "    \n",
    "#     # The \"bias\" value is always one, to allow us to assign a \"default\" score to the text\n",
    "#     features['bias'] = 1\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# feature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "604e3bb4-c41c-4fcc-aff5-788bf18adedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/amiagarw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/amiagarw/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/amiagarw/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "good_words = set()\n",
    "bad_words = set()\n",
    "\n",
    "# Feature weights\n",
    "feature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}\n",
    "\n",
    "import re\n",
    "\n",
    "def preprocess_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Preprocess text by:\n",
    "    - Correcting contractions dynamically using POS tagging\n",
    "    - Removing extra spaces and handling misplaced spaces around punctuation and apostrophes\n",
    "    - Enforcing single spaces between words\n",
    "    - Converting text to lowercase\n",
    "    \"\"\"\n",
    "    # Tokenize and POS-tag the text\n",
    "    tokens = word_tokenize(text)\n",
    "    tagged = pos_tag(tokens)\n",
    "\n",
    "    # Reconstruct contractions dynamically\n",
    "    corrected_tokens = []\n",
    "    i = 0\n",
    "    while i < len(tagged):\n",
    "        word, tag = tagged[i]\n",
    "        if word == \"'\":\n",
    "            if i > 0 and i < len(tagged) - 1:\n",
    "                prev_word, prev_tag = tagged[i - 1]\n",
    "                next_word, next_tag = tagged[i + 1]\n",
    "\n",
    "                # Handle contractions like \"it 's\" -> \"it's\"\n",
    "                if next_word in [\"s\", \"ll\", \"ve\", \"re\", \"d\", \"t\"]:\n",
    "                    corrected_tokens[-1] = prev_word + \"'\" + next_word\n",
    "                    i += 1  # Skip the next word\n",
    "                else:\n",
    "                    corrected_tokens.append(word)\n",
    "            else:\n",
    "                corrected_tokens.append(word)\n",
    "        else:\n",
    "            corrected_tokens.append(word)\n",
    "        i += 1\n",
    "\n",
    "    # Reconstruct the text\n",
    "    text = \" \".join(corrected_tokens)\n",
    "\n",
    "    # Handle misplaced spaces around punctuation (e.g., \"word ,\" -> \"word,\")\n",
    "    text = re.sub(r\"\\s+([,.!?])\", r\"\\1\", text)   # Remove space before punctuation\n",
    "    text = re.sub(r\"([,.!?])\\s+\", r\"\\1 \", text)  # Ensure space after punctuation\n",
    "\n",
    "    # Replace or handle double dashes (-- or ––)\n",
    "    text = re.sub(r\"--+\", \"—\", text)  # Convert multiple dashes to an em dash\n",
    "\n",
    "    # Remove extra spaces and enforce single spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "def create_sentiment_dictionaries(x_train: List[str], y_train: List[int]) -> None:\n",
    "    \"\"\"\n",
    "    Dynamically create dictionaries of good words and bad words from x_train and y_train.\n",
    "    Updates the global variables `good_words` and `bad_words`.\n",
    "    \"\"\"\n",
    "    global good_words, bad_words\n",
    "    good_words_counter = Counter()\n",
    "    bad_words_counter = Counter()\n",
    "\n",
    "    for text, label in zip(x_train, y_train):\n",
    "        text = preprocess_text(text)\n",
    "        words = nltk.word_tokenize(text)\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        relevant_words = filter_words_by_pos(filtered_words)\n",
    "        if label == 1:\n",
    "            good_words_counter.update(relevant_words)\n",
    "        elif label == -1:\n",
    "            bad_words_counter.update(relevant_words)\n",
    "    \n",
    "    good_words = set(good_words_counter.keys()) - set(bad_words_counter.keys())\n",
    "    bad_words = set(bad_words_counter.keys()) - set(good_words_counter.keys())\n",
    "\n",
    "def filter_words_by_pos(words: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Retain only adjectives, adverbs, verbs, and nouns to focus on sentiment-related words.\n",
    "    \"\"\"\n",
    "    relevant_pos_tags = {'JJ', 'JJR', 'JJS', 'RB', 'RBR', 'RBS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'NN', 'NNS'}\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    return [word for word, pos in tagged_words if pos in relevant_pos_tags]\n",
    "\n",
    "def extract_features(x: str) -> dict[str, float]:\n",
    "    \"\"\"\n",
    "    Extract features for sentiment analysis, including negation handling and sentence-level features.\n",
    "    \"\"\"\n",
    "    features = Counter()\n",
    "    x = preprocess_text(x)\n",
    "    words = nltk.word_tokenize(x)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    relevant_words = filter_words_by_pos(filtered_words)\n",
    "\n",
    "    # Count good and bad words\n",
    "    features['good_word_count'] = sum(1 for word in relevant_words if word in good_words)\n",
    "    features['bad_word_count'] = sum(1 for word in relevant_words if word in bad_words)\n",
    "\n",
    "    # Negation handling\n",
    "    negations = {\"not\", \"never\", \"no\"}\n",
    "    negated = False\n",
    "    for word in relevant_words:\n",
    "        if word in negations:\n",
    "            negated = not negated\n",
    "            continue\n",
    "        if negated:\n",
    "            if word in good_words:\n",
    "                features['bad_word_count'] += 1\n",
    "            elif word in bad_words:\n",
    "                features['good_word_count'] += 1\n",
    "        negated = False if word.endswith((\".\", \",\", \"!\", \"?\")) else negated\n",
    "\n",
    "    # Add sentence-level features\n",
    "    features['bias'] = 1  # Bias feature\n",
    "    features['punctuation_count'] = x.count(\",\") + x.count(\"...\")  # Punctuation\n",
    "    features['length'] = len(filtered_words) / 100.0  # Normalized sentence length\n",
    "\n",
    "    return dict(features)\n",
    "\n",
    "# Create sentiment dictionaries\n",
    "create_sentiment_dictionaries(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2a9472a-3f47-4d79-ae29-e8c885d5a2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment dictionaries dynamically from x_train and y_train\n",
    "# create_sentiment_dictionaries(x_train, y_train)\n",
    "\n",
    "# Feature weights for scoring\n",
    "# feature_weights = {'good_word_count': 1.0, 'bad_word_count': -1.0, 'bias': 0.5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0ee95ea-7e29-47ed-94a4-2e693c569c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the classifier over the training and dev (test) sets and calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5de8eaa5-5acc-4388-9ea9-8e633657bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_classifier(x: str) -> int:\n",
    "    score = 0\n",
    "    for feat_name, feat_value in extract_features(x).items():\n",
    "        score = score + feat_value * feature_weights.get(feat_name, 0)\n",
    "    if score > 0:\n",
    "        return 1\n",
    "    elif score < 0:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03f095ca-5018-43e3-b6e4-b362e5a42cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x_data: list[str], y_data: list[int]) -> float:\n",
    "    total_number = 0\n",
    "    correct_number = 0\n",
    "    for x, y in zip(x_data, y_data):\n",
    "        y_pred = run_classifier(x)\n",
    "        total_number += 1\n",
    "        if y == y_pred:\n",
    "            correct_number += 1\n",
    "    return correct_number / float(total_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5000315f-eb52-48f2-b8fc-a84661d7277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 444, 0: 229, -1: 428}\n"
     ]
    }
   ],
   "source": [
    "label_count = {}\n",
    "for y in y_test:\n",
    "    if y not in label_count:\n",
    "        label_count[y] = 0\n",
    "    label_count[y] += 1\n",
    "print(label_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4359f9fb-33aa-423d-85b3-5331cda74741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.7299859550561798\n",
      "Dev/test accuracy: 0.5004541326067211\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = calculate_accuracy(x_train, y_train)\n",
    "test_accuracy = calculate_accuracy(x_test, y_test)\n",
    "print(f'Train accuracy: {train_accuracy}')\n",
    "print(f'Dev/test accuracy: {test_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "214db0d7-5b2e-4e44-a1a6-3f81df6cdc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis\n",
    "# The following two functions allow you to randomly observe some mistaken examples, which may help you improve the classifier.\n",
    "# Feel free to write more sophisticated methods for error analysis as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5e3df7e-e7ec-4c9a-8494-b92b567a85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def find_errors(x_data, y_data):\n",
    "    error_ids = []\n",
    "    y_preds = []\n",
    "    for i, (x, y) in enumerate(zip(x_data, y_data)):\n",
    "        y_preds.append(run_classifier(x))\n",
    "        if y != y_preds[-1]:\n",
    "            error_ids.append(i)\n",
    "    for _ in range(5):\n",
    "        my_id = random.choice(error_ids)\n",
    "        x, y, y_pred = x_data[my_id], y_data[my_id], y_preds[my_id]\n",
    "        print(f'{x}\\ntrue label: {y}\\npredicted label: {y_pred}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc6cc6ce-f124-4edb-ba1b-7ac872c8027a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 's dark and tragic , and lets the business of the greedy talent agents get in the way of saying something meaningful about facing death\n",
      "true label: 0\n",
      "predicted label: 1\n",
      "\n",
      "Snipes relies too much on a scorchingly plotted dramatic scenario for its own good .\n",
      "true label: 0\n",
      "predicted label: 1\n",
      "\n",
      "Anyone who gets chills from movies with giant plot holes will find plenty to shake and shiver about in ` The Ring . '\n",
      "true label: 0\n",
      "predicted label: -1\n",
      "\n",
      "Qutting may be a flawed film , but it is nothing if not sincere .\n",
      "true label: 0\n",
      "predicted label: 1\n",
      "\n",
      "A rote exercise in both animation and storytelling .\n",
      "true label: -1\n",
      "predicted label: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "find_errors(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b487cc0c-0af7-4055-85fe-6c5659ee2ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
